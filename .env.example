# LiteLLM Configuration
# The following settings are now hardcoded:
# - Host: 0.0.0.0
# - Port: PORT environment variable (default: 4000)
# - Workers: 1
# - Debug: false
# - Detailed debug: false

# Port Configuration
PORT=4000

# Master Key Configuration
LITELLM_MASTER_KEY=sk-local-master

# Streaming Configuration
IS_STREAMING=true

# Global defaults (used when individual model entries omit values)
OPENAI_API_BASE=https://agentrouter.org/v1
OPENAI_API_KEY=sk-your-upstream-api-key  # secret read by LiteLLM and referenced by generated config

# Token Limits / other shared knobs
MAX_TOKENS=8192

# ---------------------------------------------------------------------------
# Multi-model Proxy Configuration
# ---------------------------------------------------------------------------
# Define the logical keys for each model. Keys must be alphanumeric/underscore.
# This allows running multiple models concurrently from a single proxy instance.

# Example 1: Dual-model setup (GPT-5 + DeepSeek v3.2)
# Uncomment the lines below to enable both models
# PROXY_MODEL_KEYS=gpt5,deepseek

# GPT-5 model configuration (supports reasoning effort)
# MODEL_GPT5_ALIAS=gpt-5
# MODEL_GPT5_UPSTREAM_MODEL=gpt-5
# MODEL_GPT5_REASONING_EFFORT=medium

# DeepSeek v3.2 model configuration
# MODEL_DEEPSEEK_ALIAS=deepseek-v3.2
# MODEL_DEEPSEEK_UPSTREAM_MODEL=deepseek-v3.2
# MODEL_DEEPSEEK_REASONING_EFFORT=medium

# Example 2: Single-model setup using new schema
# PROXY_MODEL_KEYS=primary
# MODEL_PRIMARY_ALIAS=gpt-5
# MODEL_PRIMARY_UPSTREAM_MODEL=gpt-5
# MODEL_PRIMARY_REASONING_EFFORT=medium

# Example 3: Single DeepSeek model
# PROXY_MODEL_KEYS=deepseek
# MODEL_DEEPSEEK_ALIAS=deepseek-v3.2
# MODEL_DEEPSEEK_UPSTREAM_MODEL=deepseek-v3.2
# MODEL_DEEPSEEK_REASONING_EFFORT=medium

# ---------------------------------------------------------------------------
# Migration Notes
# ---------------------------------------------------------------------------
# - Use PROXY_MODEL_KEYS schema even when only exposing a single model
# - Each model can optionally override API base/key vars via MODEL_<KEY>_* entries
# - Reasoning effort is automatically filtered for incompatible models
# ---------------------------------------------------------------------------


# Test-specific settings
TEST_TIMEOUT=30
TEST_MAX_TOKENS=500
