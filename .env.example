# LiteLLM Proxy - Example Environment File
# Copy to .env and customize. Do NOT commit secrets.

# ---------------------------------------------------------------------------
# Core service configuration
# ---------------------------------------------------------------------------
# Port the host will publish (docker-compose maps ${PORT:-4000} -> container 4000)
PORT=4000

# Master key used by the local proxy for client authentication
LITELLM_MASTER_KEY=sk-local-master

# Whether server responses stream by default (can be overridden per request)
STREAMING_ENABLE=true

# Optional: default reasoning effort for CLI defaults when supported by a model
# Values: none|low|medium|high
REASONING_EFFORT=medium

# ---------------------------------------------------------------------------
# Global upstream defaults
# Used when individual model entries omit base/key settings
# ---------------------------------------------------------------------------
# Base URL for upstream OpenAI-compatible APIs
OPENAI_BASE_URL=https://agentrouter.org/v1

# Upstream API key (referenced by generated config). Replace with your real key.
# IMPORTANT: This is a secret. Keep only in your local .env (never commit).
OPENAI_API_KEY=sk-your-upstream-api-key

# Token limits / other shared knobs
MAX_TOKENS=8192

# ---------------------------------------------------------------------------
# Node.js Upstream Proxy (REQUIRED)
# ---------------------------------------------------------------------------
# Enable Node.js upstream proxy (required for proper API compatibility)
# The proxy URL is auto-detected:
#   - Docker Compose: http://node-proxy:4000/v1 (separate service)
#   - Local/Single Container: http://127.0.0.1:4000/v1 (subprocess)
NODE_UPSTREAM_PROXY_ENABLE=1

# ---------------------------------------------------------------------------
# Telemetry
# ---------------------------------------------------------------------------
# Enable telemetry collection: 1 to enable, 0 to disable
TELEMETRY_ENABLE=1

# ---------------------------------------------------------------------------
# Multi-model Proxy Configuration (REQUIRED)
# Declare one or more MODEL_<KEY>_* variables; the proxy autodetects keys.
# Keys must be alphanumeric/underscore. `PROXY_MODEL_KEYS` is ignored.
# ---------------------------------------------------------------------------
# Minimal single-model setup (enabled by default in this example):
MODEL_PRIMARY_UPSTREAM_MODEL=gpt-5
# Optional per-model overrides (uncomment to customize):
# MODEL_PRIMARY_UPSTREAM_BASE=https://agentrouter.org/v1
# MODEL_PRIMARY_REASONING_EFFORT=medium

# Example: Multi-model setup (uncomment and adjust as needed)
MODEL_GPT5_UPSTREAM_MODEL=gpt-5
MODEL_GPT5_REASONING_EFFORT=medium

MODEL_DEEPSEEK_UPSTREAM_MODEL=deepseek-v3.2
MODEL_DEEPSEEK_REASONING_EFFORT=medium

MODEL_GROK_UPSTREAM_MODEL=grok-code-fast-1
MODEL_GROK_REASONING_EFFORT=high

MODEL_GLM_UPSTREAM_MODEL=glm-4.6

# ---------------------------------------------------------------------------
# Networking (container runtime)
# ---------------------------------------------------------------------------
# Host bind address (entrypoint defaults to 0.0.0.0). Usually leave as default.
# LITELLM_HOST=0.0.0.0

# ---------------------------------------------------------------------------
# Test-only knobs (used in demos/tests)
# ---------------------------------------------------------------------------
TEST_TIMEOUT=30
TEST_MAX_TOKENS=500
