# LiteLLM Proxy - Example Environment File
# Copy to .env and customize. Do NOT commit secrets.

# ---------------------------------------------------------------------------
# Core service configuration
# ---------------------------------------------------------------------------
# Port the host will publish (docker-compose maps ${PORT:-4000} -> container 4000)
PORT=4000

# Master key used by the local proxy for client authentication
LITELLM_MASTER_KEY=sk-local-master

# Whether server responses stream by default (can be overridden per request)
IS_STREAMING=true

# Optional: default reasoning effort for CLI defaults when supported by a model
# Values: none|low|medium|high
REASONING_EFFORT=medium

# ---------------------------------------------------------------------------
# Global upstream defaults
# Used when individual model entries omit base/key settings
# ---------------------------------------------------------------------------
# Base URL for upstream OpenAI-compatible APIs
OPENAI_BASE_URL=https://agentrouter.org/v1

# Upstream API key (referenced by generated config). Replace with your real key.
# IMPORTANT: This is a secret. Keep only in your local .env (never commit).
OPENAI_API_KEY=sk-your-upstream-api-key

# Token limits / other shared knobs
MAX_TOKENS=8192

# ---------------------------------------------------------------------------
# Telemetry
# ---------------------------------------------------------------------------
# Enable telemetry collection: 1 to enable, 0 to disable
TELEMETRY_ENABLE=1

# ---------------------------------------------------------------------------
# Multi-model Proxy Configuration (REQUIRED for generated config)
# Define logical keys, then per-key model settings.
# Keys must be alphanumeric/underscore.
# ---------------------------------------------------------------------------
# Minimal single-model setup (enabled by default in this example):
PROXY_MODEL_KEYS=PRIMARY
MODEL_PRIMARY_UPSTREAM_MODEL=gpt-5
# Optional per-model overrides (uncomment to customize):
# MODEL_PRIMARY_UPSTREAM_BASE=https://agentrouter.org/v1
# MODEL_PRIMARY_REASONING_EFFORT=medium

# Example: Multi-model setup (uncomment and adjust as needed)
# PROXY_MODEL_KEYS=GPT5,DEEPSEEK,GROK,GLM
# MODEL_GPT5_UPSTREAM_MODEL=gpt-5
# MODEL_GPT5_REASONING_EFFORT=medium
# MODEL_DEEPSEEK_UPSTREAM_MODEL=deepseek-v3.2
# MODEL_DEEPSEEK_REASONING_EFFORT=medium
# MODEL_GROK_UPSTREAM_MODEL=grok-code-fast-1
# MODEL_GROK_REASONING_EFFORT=high
# MODEL_GLM_UPSTREAM_MODEL=glm-4.6
# # Note: GLM models may require a different provider/base:
# # MODEL_GLM_UPSTREAM_BASE=https://open.bigmodel.cn/api/paas/v4

# ---------------------------------------------------------------------------
# Networking (container runtime)
# ---------------------------------------------------------------------------
# Host bind address (entrypoint defaults to 0.0.0.0). Usually leave as default.
# LITELLM_HOST=0.0.0.0

# ---------------------------------------------------------------------------
# Test-only knobs (used in demos/tests)
# ---------------------------------------------------------------------------
TEST_TIMEOUT=30
TEST_MAX_TOKENS=500
